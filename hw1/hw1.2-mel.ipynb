{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f384800-a9e7-4dff-afce-7e65a2fd1499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35625585-65bc-40ff-b94c-7edcba6b5bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-16.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def criterion(out, label):\n",
    "    return (label - out)**2\n",
    "\n",
    "# (data / label)\n",
    "# note floating point number *.*\n",
    "data = [(1.0,3.0), (2.0,6.0), (3.0,9.0), (4.0,12.0), (5.0,15.0), (6.0,18.0)]\n",
    "\n",
    "# requires_grad turned on\n",
    "W = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "\n",
    "X,label = data[1]\n",
    "\n",
    "Y = X*W\n",
    "loss = criterion(Y,label) ## calculationg loss\n",
    "loss.backward()   ## calculating derivative\n",
    "\n",
    "## loss = (label-X*W)**2\n",
    "## d(loss)/dw = 2*(label-X*W)*(-X)=2*4*(-2)\n",
    "\n",
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a4e4eca-b3e7-4234-a280-a144ffb67fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss: tensor([9.], grad_fn=<PowBackward0>)\n",
      "Epoch 0 - loss: tensor([34.5744], grad_fn=<PowBackward0>)\n",
      "Epoch 0 - loss: tensor([65.8435], grad_fn=<PowBackward0>)\n",
      "Epoch 0 - loss: tensor([78.7078], grad_fn=<PowBackward0>)\n",
      "Epoch 0 - loss: tensor([56.8664], grad_fn=<PowBackward0>)\n",
      "Epoch 0 - loss: tensor([20.4719], grad_fn=<PowBackward0>)\n",
      "Epoch 1 - loss: tensor([0.0446], grad_fn=<PowBackward0>)\n",
      "Epoch 1 - loss: tensor([0.1713], grad_fn=<PowBackward0>)\n",
      "Epoch 1 - loss: tensor([0.3262], grad_fn=<PowBackward0>)\n",
      "Epoch 1 - loss: tensor([0.3899], grad_fn=<PowBackward0>)\n",
      "Epoch 1 - loss: tensor([0.2817], grad_fn=<PowBackward0>)\n",
      "Epoch 1 - loss: tensor([0.1014], grad_fn=<PowBackward0>)\n",
      "Epoch 2 - loss: tensor([0.0002], grad_fn=<PowBackward0>)\n",
      "Epoch 2 - loss: tensor([0.0008], grad_fn=<PowBackward0>)\n",
      "Epoch 2 - loss: tensor([0.0016], grad_fn=<PowBackward0>)\n",
      "Epoch 2 - loss: tensor([0.0019], grad_fn=<PowBackward0>)\n",
      "Epoch 2 - loss: tensor([0.0014], grad_fn=<PowBackward0>)\n",
      "Epoch 2 - loss: tensor([0.0005], grad_fn=<PowBackward0>)\n",
      "Epoch 3 - loss: tensor([1.0940e-06], grad_fn=<PowBackward0>)\n",
      "Epoch 3 - loss: tensor([4.2022e-06], grad_fn=<PowBackward0>)\n",
      "Epoch 3 - loss: tensor([8.0010e-06], grad_fn=<PowBackward0>)\n",
      "Epoch 3 - loss: tensor([9.5652e-06], grad_fn=<PowBackward0>)\n",
      "Epoch 3 - loss: tensor([6.9081e-06], grad_fn=<PowBackward0>)\n",
      "Epoch 3 - loss: tensor([2.4881e-06], grad_fn=<PowBackward0>)\n",
      "Epoch 4 - loss: tensor([5.4275e-09], grad_fn=<PowBackward0>)\n",
      "Epoch 4 - loss: tensor([2.0875e-08], grad_fn=<PowBackward0>)\n",
      "Epoch 4 - loss: tensor([3.9728e-08], grad_fn=<PowBackward0>)\n",
      "Epoch 4 - loss: tensor([4.7695e-08], grad_fn=<PowBackward0>)\n",
      "Epoch 4 - loss: tensor([3.4584e-08], grad_fn=<PowBackward0>)\n",
      "Epoch 4 - loss: tensor([1.2238e-08], grad_fn=<PowBackward0>)\n",
      "Epoch 5 - loss: tensor([2.7512e-11], grad_fn=<PowBackward0>)\n",
      "Epoch 5 - loss: tensor([1.1005e-10], grad_fn=<PowBackward0>)\n",
      "Epoch 5 - loss: tensor([2.0464e-10], grad_fn=<PowBackward0>)\n",
      "Epoch 5 - loss: tensor([2.3283e-10], grad_fn=<PowBackward0>)\n",
      "Epoch 5 - loss: tensor([1.7826e-10], grad_fn=<PowBackward0>)\n",
      "Epoch 5 - loss: tensor([5.8208e-11], grad_fn=<PowBackward0>)\n",
      "Epoch 6 - loss: tensor([5.6843e-14], grad_fn=<PowBackward0>)\n",
      "Epoch 6 - loss: tensor([2.2737e-13], grad_fn=<PowBackward0>)\n",
      "Epoch 6 - loss: tensor([9.0949e-13], grad_fn=<PowBackward0>)\n",
      "Epoch 6 - loss: tensor([9.0949e-13], grad_fn=<PowBackward0>)\n",
      "Epoch 6 - loss: tensor([9.0949e-13], grad_fn=<PowBackward0>)\n",
      "Epoch 6 - loss: tensor([3.6380e-12], grad_fn=<PowBackward0>)\n",
      "Epoch 7 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 7 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 7 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 7 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 7 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 7 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 8 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 8 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 8 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 8 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 8 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 8 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 9 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 9 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 9 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 9 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 9 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 9 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 10 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 10 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 10 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 10 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 10 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 10 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 11 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 11 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 11 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 11 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 11 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 11 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 12 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 12 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 12 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 12 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 12 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 12 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 13 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 13 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 13 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 13 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 13 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 13 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 14 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 14 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 14 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 14 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 14 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 14 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 15 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 15 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 15 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 15 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 15 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 15 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 16 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 16 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 16 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 16 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 16 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 16 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 17 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 17 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 17 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 17 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 17 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 17 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 18 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 18 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 18 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 18 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 18 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 18 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 19 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 19 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 19 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 19 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 19 - loss: tensor([0.], grad_fn=<PowBackward0>)\n",
      "Epoch 19 - loss: tensor([0.], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "def criterion(out, label):\n",
    "    return (label - out)**2\n",
    "\n",
    "data = [(1.0,3.0), (2.0,6.0), (3.0,9.0), (4.0,12.0), (5.0,15.0), (6.0,18.0)]\n",
    "\n",
    "W = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "lr = 0.01\n",
    "temp = torch.tensor([0.0])\n",
    "\n",
    "for epoch in range(20):\n",
    "    for i, current_data in enumerate(data):\n",
    "\n",
    "        W = temp\n",
    "        W.requires_grad = True\n",
    "        X, Y = current_data\n",
    "        outputs = X*W                 #forward passing\n",
    "        loss = criterion(Y, outputs)  #loss calculation\n",
    "        loss.backward()   #backward propagation\n",
    "        W = W - lr*W.grad\n",
    "        temp = W.detach()  # not to deliver the grad, but just value\n",
    "        print(\"Epoch {} - loss: {}\" .format(epoch, loss))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a86932d-f986-46a5-a76c-c5d7bc2ee457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer1): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n",
      "[Parameter containing:\n",
      "tensor([[0.3729]], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()  ## forced to do\n",
    "        self.layer1 = nn.Linear(1, 1, bias = False)\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "print(net)\n",
    "print(list(net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c58dff4-c266-4c06-b230-ff7e3efd339c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Loss 6.901564121246338\n",
      "Epoch 0 - Loss 26.51304817199707\n",
      "Epoch 0 - Loss 49.377620697021484\n",
      "Epoch 0 - Loss 52.10383987426758\n",
      "Epoch 0 - Loss 22.955814361572266\n",
      "Epoch 0 - Loss 0.11272332817316055\n",
      "Epoch 1 - Loss 0.18963898718357086\n",
      "Epoch 1 - Loss 1.80893075466156\n",
      "Epoch 1 - Loss 4.891006946563721\n",
      "Epoch 1 - Loss 6.489166736602783\n",
      "Epoch 1 - Loss 3.665015459060669\n",
      "Epoch 1 - Loss 0.14959122240543365\n",
      "Epoch 2 - Loss 0.019926754757761955\n",
      "Epoch 2 - Loss 0.2326149344444275\n",
      "Epoch 2 - Loss 0.6651371121406555\n",
      "Epoch 2 - Loss 0.9083672761917114\n",
      "Epoch 2 - Loss 0.5273153185844421\n",
      "Epoch 2 - Loss 0.02451907843351364\n",
      "Epoch 3 - Loss 0.0027311493176966906\n",
      "Epoch 3 - Loss 0.032684486359357834\n",
      "Epoch 3 - Loss 0.09405771642923355\n",
      "Epoch 3 - Loss 0.12886950373649597\n",
      "Epoch 3 - Loss 0.07503637671470642\n",
      "Epoch 3 - Loss 0.00353726907633245\n",
      "Epoch 4 - Loss 0.0003865529433824122\n",
      "Epoch 4 - Loss 0.004638840910047293\n",
      "Epoch 4 - Loss 0.013359085656702518\n",
      "Epoch 4 - Loss 0.018309617415070534\n",
      "Epoch 4 - Loss 0.010664605535566807\n",
      "Epoch 4 - Loss 0.000503466057125479\n",
      "Epoch 5 - Loss 5.490530020324513e-05\n",
      "Epoch 5 - Loss 0.000659098441246897\n",
      "Epoch 5 - Loss 0.001898223883472383\n",
      "Epoch 5 - Loss 0.0026017415802925825\n",
      "Epoch 5 - Loss 0.0015154659049585462\n",
      "Epoch 5 - Loss 7.155622006393969e-05\n",
      "Epoch 6 - Loss 7.802592335792724e-06\n",
      "Epoch 6 - Loss 9.366149606648833e-05\n",
      "Epoch 6 - Loss 0.0002697536547202617\n",
      "Epoch 6 - Loss 0.0003697152715176344\n",
      "Epoch 6 - Loss 0.00021535974519792944\n",
      "Epoch 6 - Loss 1.017027534544468e-05\n",
      "Epoch 7 - Loss 1.109008735511452e-06\n",
      "Epoch 7 - Loss 1.3313434465089813e-05\n",
      "Epoch 7 - Loss 3.834343078779057e-05\n",
      "Epoch 7 - Loss 5.2546238293871284e-05\n",
      "Epoch 7 - Loss 3.060595190618187e-05\n",
      "Epoch 7 - Loss 1.4439137885347009e-06\n",
      "Epoch 8 - Loss 1.575827468514035e-07\n",
      "Epoch 8 - Loss 1.8911705410573632e-06\n",
      "Epoch 8 - Loss 5.445880560728256e-06\n",
      "Epoch 8 - Loss 7.465337148460094e-06\n",
      "Epoch 8 - Loss 4.346107743913308e-06\n",
      "Epoch 8 - Loss 2.06069671548903e-07\n",
      "Epoch 9 - Loss 2.2418134904000908e-08\n",
      "Epoch 9 - Loss 2.691522240638733e-07\n",
      "Epoch 9 - Loss 7.74824911786709e-07\n",
      "Epoch 9 - Loss 1.0628000381984748e-06\n",
      "Epoch 9 - Loss 6.190248313941993e-07\n",
      "Epoch 9 - Loss 2.9467628337442875e-08\n",
      "Epoch 10 - Loss 3.1928379939927254e-09\n",
      "Epoch 10 - Loss 3.84081886295462e-08\n",
      "Epoch 10 - Loss 1.1077736417064443e-07\n",
      "Epoch 10 - Loss 1.5139812603592873e-07\n",
      "Epoch 10 - Loss 8.796723705017939e-08\n",
      "Epoch 10 - Loss 4.20550350099802e-09\n",
      "Epoch 11 - Loss 4.604316927725449e-10\n",
      "Epoch 11 - Loss 5.46265255252365e-09\n",
      "Epoch 11 - Loss 1.560783857712522e-08\n",
      "Epoch 11 - Loss 2.1569576347246766e-08\n",
      "Epoch 11 - Loss 1.2663804227486253e-08\n",
      "Epoch 11 - Loss 6.148184183984995e-10\n",
      "Epoch 12 - Loss 6.571099220309407e-11\n",
      "Epoch 12 - Loss 7.914877642178908e-10\n",
      "Epoch 12 - Loss 2.2737367544323206e-09\n",
      "Epoch 12 - Loss 3.165951056871563e-09\n",
      "Epoch 12 - Loss 1.8417267710901797e-09\n",
      "Epoch 12 - Loss 9.094947017729282e-11\n",
      "Epoch 13 - Loss 9.606537787476555e-12\n",
      "Epoch 13 - Loss 1.2028067430946976e-10\n",
      "Epoch 13 - Loss 3.637978807091713e-10\n",
      "Epoch 13 - Loss 4.81122697237879e-10\n",
      "Epoch 13 - Loss 2.9467628337442875e-10\n",
      "Epoch 13 - Loss 1.4551915228366852e-11\n",
      "Epoch 14 - Loss 2.0463630789890885e-12\n",
      "Epoch 14 - Loss 2.2737367544323206e-11\n",
      "Epoch 14 - Loss 5.820766091346741e-11\n",
      "Epoch 14 - Loss 9.094947017729282e-11\n",
      "Epoch 14 - Loss 5.820766091346741e-11\n",
      "Epoch 14 - Loss 3.637978807091713e-12\n",
      "Epoch 15 - Loss 5.115907697472721e-13\n",
      "Epoch 15 - Loss 5.6843418860808015e-12\n",
      "Epoch 15 - Loss 1.4551915228366852e-11\n",
      "Epoch 15 - Loss 1.4551915228366852e-11\n",
      "Epoch 15 - Loss 3.637978807091713e-12\n",
      "Epoch 15 - Loss 0.0\n",
      "Epoch 16 - Loss 5.684341886080802e-14\n",
      "Epoch 16 - Loss 2.2737367544323206e-13\n",
      "Epoch 16 - Loss 9.094947017729282e-13\n",
      "Epoch 16 - Loss 9.094947017729282e-13\n",
      "Epoch 16 - Loss 9.094947017729282e-13\n",
      "Epoch 16 - Loss 0.0\n",
      "Epoch 17 - Loss 0.0\n",
      "Epoch 17 - Loss 0.0\n",
      "Epoch 17 - Loss 0.0\n",
      "Epoch 17 - Loss 0.0\n",
      "Epoch 17 - Loss 0.0\n",
      "Epoch 17 - Loss 0.0\n",
      "Epoch 18 - Loss 0.0\n",
      "Epoch 18 - Loss 0.0\n",
      "Epoch 18 - Loss 0.0\n",
      "Epoch 18 - Loss 0.0\n",
      "Epoch 18 - Loss 0.0\n",
      "Epoch 18 - Loss 0.0\n",
      "Epoch 19 - Loss 0.0\n",
      "Epoch 19 - Loss 0.0\n",
      "Epoch 19 - Loss 0.0\n",
      "Epoch 19 - Loss 0.0\n",
      "Epoch 19 - Loss 0.0\n",
      "Epoch 19 - Loss 0.0\n",
      "when x = tensor([1.]), pred_y = tensor([3.], grad_fn=<SqueezeBackward4>), y = tensor([3.])\n",
      "when x = tensor([2.]), pred_y = tensor([6.], grad_fn=<SqueezeBackward4>), y = tensor([6.])\n",
      "when x = tensor([3.]), pred_y = tensor([9.], grad_fn=<SqueezeBackward4>), y = tensor([9.])\n",
      "when x = tensor([4.]), pred_y = tensor([12.], grad_fn=<SqueezeBackward4>), y = tensor([12.])\n",
      "when x = tensor([5.]), pred_y = tensor([15.], grad_fn=<SqueezeBackward4>), y = tensor([15.])\n",
      "when x = tensor([6.]), pred_y = tensor([18.], grad_fn=<SqueezeBackward4>), y = tensor([18.])\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr = 0.01, momentum = 0.5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "data = [(1.0, 3.0), (2.0, 6.0), (3.0, 9.0), (4.0, 12.0), (5.0, 15.0), (6.0, 18.0)]\n",
    "\n",
    "for epoch in range(20):\n",
    "    for i, current_data in enumerate(data):\n",
    "        X, Y = current_data\n",
    "        X, Y = torch.FloatTensor([X]), torch.FloatTensor([Y])\n",
    "        optimizer.zero_grad()   # Stochastic Gradient Decent\n",
    "        outputs = net(X)\n",
    "        loss = criterion(Y, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"Epoch {} - Loss {}\" .format(epoch, loss))\n",
    "\n",
    "\n",
    "for i, current_data in enumerate(data):\n",
    "    X, Y = current_data\n",
    "    X, Y = torch.FloatTensor([X]), torch.FloatTensor([Y])\n",
    "    out = net(X)\n",
    "    print(\"when x = {}, pred_y = {}, y = {}\" .format(X, out, Y))\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755e291d-c06e-4dd5-95b6-b46b4b94aade",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
